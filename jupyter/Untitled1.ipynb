{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_index(item, lst):\n",
    "    enumerated = list(enumerate(lst))\n",
    "    def get_index_step(item, enumerated):\n",
    "        if len(enumerated) == 1:\n",
    "            return enumerated[0][0]\n",
    "        middle_index = math.floor(len(enumerated)/2)\n",
    "        if enumerated[middle_index][1] > item:\n",
    "            get_index_step(item, enumerated[:middle_index])\n",
    "        elif enumerated[middle_index][1] == item:\n",
    "            return(True #(enumerated[middle_index]))\n",
    "        else:\n",
    "            get_index_step(item, enumerated[middle_index:])\n",
    "    print(get_index_step(item, enumerated))\n",
    "    return get_index_step(item, enumerated)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(get_index(8,[2,3,4, 8,12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (1, 3), (2, 4), (3, 8), (4, 12)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate([2,3,4, 8,12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(A, B):\n",
    "    merged = []\n",
    "    while len(A)>0:\n",
    "        a = A.pop(0)\n",
    "        while len(B)>0:\n",
    "            b = B.pop(0)\n",
    "            if b <= a:\n",
    "                merged.append(b)\n",
    "            else:\n",
    "                merged.append(a)\n",
    "                break\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_sort([3,5,7], [1,8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = sklearn.datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_data['data'], iris_data['target'], test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/adam/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = sklearn.linear_model.LogisticRegression()\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n",
      "0.6 1.0\n",
      "1.0 0.8333333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      0.60      0.75        30\n",
      "           2       0.67      1.00      0.80        24\n",
      "\n",
      "    accuracy                           0.84        75\n",
      "   macro avg       0.89      0.87      0.85        75\n",
      "weighted avg       0.89      0.84      0.84        75\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       0.88      1.00      0.94        30\n",
      "           2       1.00      0.83      0.91        24\n",
      "\n",
      "    accuracy                           0.95        75\n",
      "   macro avg       0.96      0.94      0.95        75\n",
      "weighted avg       0.95      0.95      0.95        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for target in range(3): \n",
    "    idx = y_test == target\n",
    "    log_predicted = log_model.predict(X_test)\n",
    "    nb_predicted = nb_model.predict(X_test)\n",
    "    log_accuracy = sum(log_predicted[idx] == y_test[idx])/len(log_predicted[idx])\n",
    "    nb_accuracy =  sum(nb_predicted[idx] == y_test[idx])/len(nb_predicted[idx])\n",
    "    print(log_accuracy, nb_accuracy)\n",
    "\n",
    "predicted_log = log_model.predict(X_test) \n",
    "print(classification_report(y_test, predicted_log))\n",
    "predicted_nb = nb_model.predict(X_test)\n",
    "print(classification_report(y_test, predicted_nb ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.88235294, 0.11764706],\n",
       "       [0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "nb_cm = confusion_matrix(nb_model.predict(X_test), y_test)\n",
    "nb_cm = nb_cm / nb_cm.sum(axis=1)[:, np.newaxis]\n",
    "nb_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.33333333, 0.66666667]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cm = confusion_matrix(log_model.predict(X_test), y_test)\n",
    "log_cm = log_cm / log_cm.sum(axis=1)[:, np.newaxis]\n",
    "log_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.88235294, 0.11764706],\n",
       "       [0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21,  0,  0],\n",
       "       [ 0, 18,  0],\n",
       "       [ 0, 12, 24]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cm = confusion_matrix(log_model.predict(X_test), y_test)\n",
    "log_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21],\n",
       "       [18],\n",
       "       [36]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cm.sum(axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
